from collections import Counter
from pathlib import Path
import torch
from torchtext.vocab import GloVe
from torchtext.vocab import Vocab
from typing import List
from typing import Union

from synthetic_ra.data.constants import START_TOKEN
from synthetic_ra.data.constants import END_TOKEN
from synthetic_ra.data.constants import PAD_TOKEN
from synthetic_ra.data.titles import RAPostTitle


GLOVE_EMB_SIZE: int = 200


def xavier_init(tensor):
    torch.nn.init.xavier_uniform_(tensor.unsqueeze(0))
    return tensor.squeeze(0)


def build_vocab_w_glove_init_from_post_titles(
    titles: List[RAPostTitle],
    max_read_samples: int = 100000,
    extra_tokens: List[str] = [START_TOKEN, END_TOKEN, PAD_TOKEN],
) -> GloVe:
    """
        Builds a vocabulary object from the provided post titles.
        Also provides an embedding initalization tensor for the token ids
        from GloVe.
    """

    counter: Counter = Counter()
    counter.update(extra_tokens)

    with open(str(dataset_file), 'r') as dataset_file:
        reader = csv.reader(dataset_file)
        headers: List[str] = next(reader)
        t: int = 0

        while t < max_read_samples:


    glove_vocab: GloVe = GloVe(
        name='6B',
        dim=GLOVE_EMB_SIZE,
        unk_init=xavier_init
    )


    # add our extra tokens in.
    for token in extra_tokens:
        with torch.no_grad():
            glove_vocab.stoi[token] = len(glove_vocab.itos)
            glove_vocab.itos.append(token)
            glove_vocab.vectors: torch.Tensor = torch.cat(
                (
                    glove_vocab.vectors,
                    torch.nn.Embedding(1, GLOVE_EMB_SIZE).weight,
                ),
                axis=0,
            )

    return glove_vocab
